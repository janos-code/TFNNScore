% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 21-Apr-2015 13:23:01
%
% This script assumes these variables are defined:
%
%   p - input data.
%   t - target data.
%load('origDataPlusBadReduceClassify.mat')
% load('coulombvdWreducegoodbind.mat')
load('vdwmc10_28_16.mat')
load('testdatavdwcoulomb.mat')
% p = p(:,ind_missClass);
% t = t(:,ind_missClass);


%inputNamesvdW
%lv_out = [ 2    14    23    27    37    40    42    43    45    67    68 ...
% 76    81    90    99   100   103   108   111   112   113   114 ...
% 128   130   131   132   137   138   142   145   146   147   152 ...
% 164   173   178   184   196   197   198   200   205   208   209 ...
% 211   227   239  ];
lv_out = [ 2    14    23    27    37    40    42    43    45    67    68 ...
76    81    90    99   100   103   108   111   112   113   114 ...
128   130   131   132   137   138   142   145   146   147   152 ...
164   173   178   184   196   197   198   200   205   208   209 ...
211   227   239  5    18    36    41    79   133   166   172   ...
204   210   240  73    89   165   175    56   107   153   186 ...
230    6   188    105   91   117   174]; 
% lv_out = [     2     5     6    12    14    18    23    27    35    36    37 ...
%    40    41    42    43    45    52    56    67    68    69    71 ...
%    73    74    75    76    79    81    89    90    91    93    99 ...
%    100   103   105   107   108   111   112   113   114   117   125 ...
%    126   128   130   131   132   133   137   138   142   144   145 ...
%    146   147   152   164   165   166   172   173   174   175   176 ...
%    178   184   186   188   196   197   198   200   204   205   208 ...
%    209   210   211   222   227   229   230   239   240    49   153 ...
%    214   46    59   187   190    95   141   228    38    94    21];
% lv_out = [ 2    14    23    27    37    40    42    43    45    67    68 ...
% 76    81    90    99   100   103   108   111   112   113   114 ...
% 128   130   131   132   137   138   142   145   146   147   152 ...
% 164   173   178   184   196   197   198   200   205   208   209 ...
% 211   227   239  5    18    36    41    79   133   166   172   ...
% 204   210   240  73    89   165   175    56   107   153   186 ...
% 230    6   188    105   91   117   174   12    35   126   38 ...
% 144   214   229   69    74    75   125   21   222   58   220 ...
% 71    52    93    95    31    59   176   187   97   140   13 ...
% 46    66    92   190   228   149   155   237   32    39    54 ...
% 124   141   65   61     88   87   170    51    62    94   192 ...
% 194   203   49    50   44   109   118   136   139    151   72 ...
% 77    78    80    82    83  106   110   115   129   159   167 ...
% 169   191   195   221   225   241  47    60   216   53    17 ...
% 19   163    160   223   182   234   236  185   224  4    20 ...
% 26    30    33    55    57    85    98   101   119   120   121 ...
% 127   148   150   157   161   171   177   180   181   183   189 ...
% 193   201   206   215   218   219   232   243   10    16    64 ...
% 86   104   116   156   162   199   217   226   238   244   11  ...
% 84   168   179   207   1     8     9    22    24    25   29 ...
% 34    96   123   143   154   158   122];
%       lv_out = [];
%       lv_out = [lv_out remove1(:)'];

p_orig = p;
numinputs = size(p,1);
p_orig(lv_out,:)=[];
t_orig = t(1,:);
x = p;
[r,q] = size(x);
ew = ones(size(t));
%ew(ind_missClass) = 1;
ew_orig = ew;

num_networks = 10;
num_montecarlo = 100;
frac = 0.999;

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

% Create a Single Layer Network

net = network;
net.numLayers = 1;
net.biasConnect = true(1,1);
net.layerConnect = 0;
net.layers{1}.name = 'Output';
net.layers{1}.size = 1;
net.layers{1}.transferFcn = 'logsig';
net.layers{1}.initFcn = 'initnw';
  
  % Inputs
net.numInputs = 1;
net.inputConnect(1,1) = true;
% net.inputs{1}.processFcns = {'removeconstantrows','mapminmax'};
net.inputs{1}.processFcns = {'mapminmax'};
  
  % Outputs
net.outputConnect(1) = true;
net.outputs{1}.processFcns = {};
  
  % Training
net.dividefcn = 'dividerand';
net.trainFcn = 'trainscg';
net.performFcn = 'mse';
net.trainParam.epochs = 50;

  % Adaption
net.adaptFcn = 'adaptwb';
net.inputWeights{1,1}.learnFcn = 'learngdm';
net.biases{1}.learnFcn = 'learngdm';
  
% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
% net.divideParam.trainRatio = 70/100;
% net.divideParam.valRatio = 15/100;
% net.divideParam.testRatio = 15/100;
net.divideParam.trainRatio = 70/85;
net.divideParam.valRatio = 15/85;
net.divideParam.testRatio = 0/100;
testRatio = 15/100;


% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
%net.performFcn = 'crossentropy';  % Cross-Entropy

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotconfusion', 'plotroc'};

percentErrors = zeros(1,num_montecarlo);
rocarea = zeros(1,num_montecarlo);
cmind = cell(1,num_montecarlo);
test_ind = cell(1,num_montecarlo);
roc_area = zeros(1,num_montecarlo);
IW = [];
net_com_mc = cell(1,num_montecarlo);
for j = 1:num_montecarlo,
    x = p_orig;
    t = t_orig;
    ew = ew_orig;
    net_committee = cell(1,num_networks);
    tr_comm = cell(1,num_networks);
    
    % Remove testRatio of the data for testing
    randInd = randperm(q);
    testInd = randInd(1:round(q*testRatio));
    test_ind{j} = testInd;
    xtest = x(:,testInd);
    ttest = t(:,testInd);
    ewtest = ew(:,testInd);
    x(:,testInd) = [];
    t(:,testInd) = [];
    ew(:,testInd) = [];

    % Train a committee of networks
    for i=1:num_networks,
        net_committee{i} = net;
        net_committee{i} = init(net_committee{i});
        [net_committee{i},tr_comm{i}] = train(net_committee{i},x,t,[],[],ew);
        IW = [IW; net_committee{i}.IW{1,1}];
    end
    net_com_mc{j} = net_committee;

    ytottest = [];
    % Test the Network on the test set, and find the percent error
    for i=1:num_networks,
        ytest = net_committee{i}(xtest);
        ytottest = [ytottest;ytest>0.5];
    end
    if num_networks>1,
        decisiontest = sum(ytottest)>=(num_networks/2);
    else
        decisiontest = ytottest;
    end
    
    
    percentErrors(j) = sum(decisiontest~=ttest(1,:))/length(decisiontest);
    
    ytot = [];
    ytotroc = [];
    % Test the Network on the full data set, and find the missclassified
    % data
    for i=1:num_networks,
        y = net_committee{i}(p_orig);
        ytot = [ytot;y>0.5];
        ytotroc = [ytotroc;y];
    end
    ytotroc = sum(ytotroc,1)/num_networks;
    if num_networks > 1,
        decision = sum(ytot)>=(num_networks/2);
    else
        decision = ytot;
    end
    
    [~,~,cmind{j}] = confusion(t_orig(1,:),decision);
    [tpr,fpr] = roc(t_orig(1,:),ytotroc);
    roc_area(j) = trapz(fpr,tpr);
end

